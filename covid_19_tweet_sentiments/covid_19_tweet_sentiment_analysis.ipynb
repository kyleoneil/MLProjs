{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkFrG8AVa3aU",
        "outputId": "b13692be-ae1a-43a8-c9e1-a084a34154ac"
      },
      "outputs": [],
      "source": [
        "# #@title Download Kaggle API token from Google Drive\n",
        "# import gdown\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "\n",
        "# gdrive_url = 'https://drive.google.com/uc?id=1P6t7O8vOjmQi3rMX0KQlgO6BT0fk2dI-'\n",
        "# if not os.path.exists('kaggle.json'):\n",
        "#   gdown.download(gdrive_url, 'kaggle.json', quiet=False)\n",
        "\n",
        "# kaggle_dir = Path.home() / '.kaggle'\n",
        "# kaggle_token_path = kaggle_dir / 'kaggle.json'\n",
        "\n",
        "# if not kaggle_token_path.exists():\n",
        "#   kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "#   os.rename('kaggle.json', kaggle_token_path)\n",
        "#   kaggle_token_path.chmod(0o600)\n",
        "\n",
        "# dataset_path = 'global_covid-19'\n",
        "# if not os.path.exists(dataset_path):\n",
        "#   os.mkdir(dataset_path)\n",
        "#   !kaggle datasets download -d rohitashchandra/global-covid19-twitter-dataset --path {dataset_path} --unzip\n",
        "#   gdrive_url = 'https://drive.google.com/uc?id=19Hr0PQSyqGqLLJlPaDKjMfvlbFVnYbBr'\n",
        "#   os.remove(os.path.join(dataset_path, 'India.csv'))\n",
        "#   gdown.download(gdrive_url, os.path.join(dataset_path, 'India.csv'), quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from csv import QUOTE_NONE\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from geopy.geocoders import Nominatim\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "Tw3wDP_tkhDf",
        "outputId": "9dbe4be1-d1cc-4539-dc44-dbc5404963b1"
      },
      "outputs": [],
      "source": [
        "#@title merge into one csv\n",
        "\n",
        "# List to hold the dataframes\n",
        "dataframes = []\n",
        "dataset = ['Australia.csv', 'Brazil.csv', 'India.csv', 'Indonesia.csv', 'Japan.csv']\n",
        "# Read each CSV file with specific settings to handle quoting issues\n",
        "for data in dataset:\n",
        "  try:\n",
        "    # Open the file with utf-8 encoding, handle errors by replacing problematic characters\n",
        "    with open(data, 'r', encoding='utf-8', newline='') as file:\n",
        "      df = pd.read_csv(file, on_bad_lines='skip', quoting=csv.QUOTE_MINIMAL)\n",
        "      df = df.iloc[:, 1:]\n",
        "  except UnicodeDecodeError:\n",
        "    # Fallback to a different encoding if UTF-8 doesn't work\n",
        "    with open(data, 'r', encoding='ISO-8859-1', newline='') as file:\n",
        "      df = pd.read_csv(file, on_bad_lines='skip', quoting=csv.QUOTE_MINIMAL)\n",
        "      df = df.iloc[:, 1:]\n",
        "  dataframes.append(df)\n",
        "  random_rows = df.sample(n=3)\n",
        "  # Display the random rows\n",
        "  print(f\"Random rows from {data}:\")\n",
        "  display(random_rows)\n",
        "\n",
        "# Concatenate all dataframes\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined dataframe to a new CSV file\n",
        "combined_df.to_csv('tweets.csv', index=False)\n",
        "\n",
        "\n",
        "# open and sample 20 rows from tweets.csv\n",
        "df = pd.read_csv('tweets.csv')\n",
        "random_rows = df.sample(n=20)\n",
        "print(f\"Random rows from {df}:\")\n",
        "display(random_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('tweets.csv')\n",
        "df.head()\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming df is your DataFrame\n",
        "df['text'] = df['text'].astype(str)\n",
        "# Remove URLs\n",
        "df['cleaned_text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "# Remove mentions\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
        "# Remove hashtags\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'#\\w+', '', x))\n",
        "# Remove punctuation\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "# Remove 'RT'\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() != 'rt']))\n",
        "# Tokenize text\n",
        "df['tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
        "# Lemmatize tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_valid_date(date_str):\n",
        "    try:\n",
        "        pd.to_datetime(date_str)\n",
        "        return True\n",
        "    except (ValueError, OverflowError):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count and drop invalid dates\n",
        "def count_and_drop_invalid_dates(df, column):\n",
        "    # Identify valid dates\n",
        "    valid_dates = df[column].apply(is_valid_date)\n",
        "    \n",
        "    # Count invalid dates\n",
        "    invalid_date_count = (~valid_dates).sum()\n",
        "    \n",
        "    # Drop rows with invalid dates\n",
        "    df_valid = df[valid_dates].reset_index(drop=True)\n",
        "    \n",
        "    return df_valid, invalid_date_count\n",
        "\n",
        "# Apply the function\n",
        "df, invalid_date_count = count_and_drop_invalid_dates(df, 'created_at')\n",
        "\n",
        "print(f\"Number of rows with invalid dates: {invalid_date_count}\")\n",
        "\n",
        "# Display the processed DataFrame\n",
        "print(\"Processed DataFrame without invalid dates:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y')\n",
        "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
        "df['month'] = df['created_at'].dt.month\n",
        "df['year'] = df['created_at'].dt.year\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['text_length'] = df['cleaned_text'].apply(len)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['cleaned_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the lexicon\n",
        "lexicon = pd.read_csv('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', sep='\\t', names=['word', 'emotion', 'association'])\n",
        "lexicon_pivot = lexicon.pivot(index='word', columns='emotion', values='association').fillna(0)\n",
        "lexicon_pivot.index = lexicon_pivot.index.map(str)  # Ensure index is all strings\n",
        "vocabulary_dict = {word: i for i, word in enumerate(lexicon_pivot.index)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.dropna()\n",
        "\n",
        "# Define a more nuanced emotion mapping\n",
        "emotion_mapping = {\n",
        "    'anger': ['anger'],\n",
        "    'fear': ['fear'],\n",
        "    'joy': ['joy'],\n",
        "    'worried': ['fear', 'sadness'],\n",
        "    'sadness': ['sadness'],\n",
        "    'disgust': ['disgust'],\n",
        "    'trust': ['trust'],\n",
        "    'anticipation': ['anticipation'],\n",
        "    'anxiety': ['fear', 'anticipation', 'negative']  # New definition for anxiety\n",
        "}\n",
        "\n",
        "# Vectorize tweets\n",
        "vectorizer = CountVectorizer(vocabulary=vocabulary_dict)\n",
        "df['cleaned_text'] = df['cleaned_text'].fillna('').astype(str)\n",
        "tweet_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate emotion scores\n",
        "emotion_scores = np.zeros((df.shape[0], len(emotion_mapping)))\n",
        "\n",
        "for i, (category, emotions) in enumerate(emotion_mapping.items()):\n",
        "    indices = np.array([lexicon_pivot.columns.get_loc(emotion) for emotion in emotions if emotion in lexicon_pivot.columns])\n",
        "    emotion_scores[:, i] += tweet_matrix.dot(lexicon_pivot.iloc[:, indices].sum(axis=1))\n",
        "\n",
        "# Normalize scores\n",
        "emotion_scores_sum = emotion_scores.sum(axis=1, keepdims=True)\n",
        "emotion_scores = np.where(emotion_scores_sum > 0, emotion_scores / emotion_scores_sum, 0)\n",
        "\n",
        "# Assign the dominant emotion\n",
        "dominant_emotion = pd.DataFrame(emotion_scores, columns=emotion_mapping.keys()).idxmax(axis=1)\n",
        "df['emotion'] = np.where(emotion_scores_sum.squeeze() > 0, dominant_emotion, 'neutral')  # Default to 'neutral'\n",
        "\n",
        "# Save results\n",
        "df.to_csv('labeled_tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the number of non-zero entries in the vectorized matrix\n",
        "non_zero_matrix_entries = np.count_nonzero(tweet_matrix.sum(axis=1))\n",
        "\n",
        "# Calculate the percentage of tweets with non-zero entries\n",
        "percentage_non_zero = (non_zero_matrix_entries / df.shape[0]) * 100\n",
        "print(f\"Percentage of tweets containing lexicon words: {percentage_non_zero}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the raw emotion scores for a random sample of tweets\n",
        "sample_scores = pd.DataFrame(emotion_scores, columns=emotion_mapping.keys())\n",
        "print(\"Sample emotion scores:\\n\", sample_scores.sample(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape\n",
        "labeled_df = pd.read_csv('labeled_tweets.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text into a single string\n",
        "labeled_df['cleaned_text'] = labeled_df['cleaned_text'].astype(str)\n",
        "text = \" \".join(review for review in labeled_df.cleaned_text)\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(text)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
